{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "from typing import List,Iterator,Tuple\n",
    "from pyspark.sql.pandas.functions import pandas_udf\n",
    "from pyspark.sql.functions import struct, col\n",
    "\n",
    "from pyspark.sql.types import StringType,DoubleType\n",
    "from sparkstudy.deploy.demo_sessions import DemoSQLSessionFactory\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "COLUMNS = [\"name\",\"age\",\"salary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "比较下开不开启arrow的区别\n",
    "\n",
    "测试下来，感觉性能提升有点奇怪。有时候会快，有时候会慢。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def create_random_data(row_num:int)->List[tuple]:\n",
    "     result = list()\n",
    "     a_str = string.ascii_uppercase\n",
    "     for i in range(row_num):\n",
    "         random_letter = random.choice(a_str)\n",
    "         result.append((random_letter,random.randint(1,row_num),random.random()))\n",
    "     return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def test_performance(session_factory:DemoSQLSessionFactory, n:int = 100000):\n",
    "    data = create_random_data(n)\n",
    "    spark_session = session_factory.build_session()\n",
    "    df = spark_session.createDataFrame(data,COLUMNS).cache()\n",
    "    df.toPandas().head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.33 s\n"
     ]
    }
   ],
   "source": [
    "session_factory_normal = DemoSQLSessionFactory(name=\"normal\")\n",
    "%time test_performance(session_factory_normal)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.01 s\n"
     ]
    }
   ],
   "source": [
    "session_factory_arrow = DemoSQLSessionFactory(name=\"with arraw\")\n",
    "session_factory_arrow.add_config(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")\n",
    "%time test_performance(session_factory_arrow)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "常规的HelloWorld的example。\n",
    "页面上面的第一个例子。本质就是生成一个新的dataframe\n",
    "1. 在annotation上面列出的是新的dataframe的col和类型\n",
    "2. 他会自动的把pd的转换成spark的\n",
    "3. 函数应该会分批node执行。然后再汇总。因为我看到了。hello world的函数会被执行好几次"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-------------------+\n",
      "|name|age|             salary|\n",
      "+----+---+-------------------+\n",
      "|   B| 53|0.07222414671306154|\n",
      "|   S|284| 0.7591491065150204|\n",
      "|   W|795| 0.1321503938480999|\n",
      "|   H|817|0.23527904716797643|\n",
      "|   P|391|   0.56607862908039|\n",
      "|   D|517| 0.9009097520324495|\n",
      "|   C|657| 0.6629804651945165|\n",
      "|   U|427| 0.5658856496492505|\n",
      "|   D|702| 0.8122834294088883|\n",
      "|   O| 53| 0.9928525356156671|\n",
      "|   F|612| 0.4957293923077204|\n",
      "|   C|716| 0.4814493693803298|\n",
      "|   L|279|0.23051910331483128|\n",
      "|   Y|626|0.03737641069339437|\n",
      "|   D|965|     0.981804320733|\n",
      "|   X|756|  0.729066895773649|\n",
      "|   T|430| 0.4098659049818115|\n",
      "|   J|565|0.38979064683209386|\n",
      "|   T|751| 0.4876784057142557|\n",
      "|   H| 35| 0.3326300009380576|\n",
      "+----+---+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session_factory_arrow.add_config('spark.sql.execution.arrow.maxRecordsPerBatch',10)\n",
    "spark = session_factory_arrow.build_session()\n",
    "test_data = create_random_data(row_num=1000)\n",
    "basic_df = spark.createDataFrame(test_data,COLUMNS)\n",
    "basic_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              result|\n",
      "+--------------------+\n",
      "| [53.07222414671306]|\n",
      "|  [284.759149106515]|\n",
      "| [795.1321503938481]|\n",
      "|  [817.235279047168]|\n",
      "| [391.5660786290804]|\n",
      "| [517.9009097520325]|\n",
      "| [657.6629804651946]|\n",
      "|[427.56588564964926]|\n",
      "| [702.8122834294089]|\n",
      "| [53.99285253561567]|\n",
      "| [612.4957293923077]|\n",
      "| [716.4814493693804]|\n",
      "| [279.2305191033148]|\n",
      "| [626.0373764106934]|\n",
      "|  [965.981804320733]|\n",
      "| [756.7290668957736]|\n",
      "| [430.4098659049818]|\n",
      "| [565.3897906468321]|\n",
      "| [751.4876784057143]|\n",
      "| [35.33263000093806]|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"total double\")\n",
    "def func(s1: pd.Series, s2: pd.Series) -> pd.DataFrame:\n",
    "    print(\"execute\")\n",
    "    s3 = pd.DataFrame()\n",
    "    s3['total'] = s1 + s2\n",
    "    return s3\n",
    "basic_df.select(func(\"age\",\"salary\").alias(\"result\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "主要是想要看看。select方法，不能不能接受一个List"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "|age|             salary|\n",
      "+---+-------------------+\n",
      "| 53|0.07222414671306154|\n",
      "|284| 0.7591491065150204|\n",
      "|795| 0.1321503938480999|\n",
      "|817|0.23527904716797643|\n",
      "|391|   0.56607862908039|\n",
      "|517| 0.9009097520324495|\n",
      "|657| 0.6629804651945165|\n",
      "|427| 0.5658856496492505|\n",
      "|702| 0.8122834294088883|\n",
      "| 53| 0.9928525356156671|\n",
      "|612| 0.4957293923077204|\n",
      "|716| 0.4814493693803298|\n",
      "|279|0.23051910331483128|\n",
      "|626|0.03737641069339437|\n",
      "|965|     0.981804320733|\n",
      "|756|  0.729066895773649|\n",
      "|430| 0.4098659049818115|\n",
      "|565|0.38979064683209386|\n",
      "|751| 0.4876784057142557|\n",
      "| 35| 0.3326300009380576|\n",
      "+---+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def to_str_func(s1: pd.Series) -> pd.Series:\n",
    "    return s1.astype(dtype=str)\n",
    "to_str = pandas_udf(to_str_func, returnType=StringType())\n",
    "\n",
    "age_c = to_str(\"age\").alias(\"age\")\n",
    "salary_c = to_str(\"salary\").alias(\"salary\")\n",
    "selects = [age_c,salary_c]\n",
    "basic_df.select(selects).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "测试以下。如果参数是不定的行不行\n",
    "\n",
    "简单的来书，\n",
    "- 确定的column个数，用Series\n",
    "- 不确定用dataframe\n",
    "- iterator是类似用流"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|            result|\n",
      "+------------------+\n",
      "| 3.827879775792262|\n",
      "| 215.5983462502658|\n",
      "|105.05956310923942|\n",
      "|192.22298153623674|\n",
      "|221.33674397043248|\n",
      "| 465.7703418007764|\n",
      "| 435.5781656327973|\n",
      "|241.63317240022997|\n",
      "| 570.2229674450396|\n",
      "| 52.62118438763036|\n",
      "|303.38638809232486|\n",
      "| 344.7177484763161|\n",
      "| 64.31482982483793|\n",
      "|23.397633094064876|\n",
      "|  947.441169507345|\n",
      "| 551.1745732048787|\n",
      "|176.24233914217896|\n",
      "|220.23171546013302|\n",
      "|  366.246482691406|\n",
      "|11.642050032832017|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"double\")\n",
    "def to_sum_func(data: pd.DataFrame) -> pd.Series:\n",
    "    return data.age*data.salary\n",
    "cols = [col(\"age\"),col(\"salary\")]\n",
    "headers = struct(cols)\n",
    "#my_sum = pandas_udf(to_sum_func, returnType=DoubleType())\n",
    "basic_df.select(to_sum_func(headers).alias(\"result\")).show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "能不能用于SQL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "basic_df.createOrReplaceTempView(\"pandas_udf\")\n",
    "spark.udf.register(\"pandas_to_str\", to_str)\n",
    "spark.sql(\"select pandas_to_str(age) from pandas_udf\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "basic_df.createOrReplaceTempView(\"pandas_udf\")\n",
    "spark.udf.register(\"pandas_to_str\", to_str)\n",
    "spark.sql(\"select pandas_to_str(age) from pandas_udf\").show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}