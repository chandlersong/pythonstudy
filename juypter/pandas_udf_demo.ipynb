{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import struct, col\n",
    "from pyspark.sql.pandas.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "\n",
    "from sparkstudy.deploy.demo_sessions import DemoSQLSessionFactory\n",
    "from sparkstudy.libs.tools import create_random_data\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "COLUMNS = [\"name\",\"age\",\"salary\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "比较下开不开启arrow的区别\n",
    "\n",
    "测试下来，感觉性能提升有点奇怪。有时候会快，有时候会慢。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def test_performance(session_factory:DemoSQLSessionFactory, n:int = 100000):\n",
    "    data = create_random_data(n)\n",
    "    spark_session = session_factory.build_session()\n",
    "    df = spark_session.createDataFrame(data,COLUMNS).cache()\n",
    "    df.toPandas().head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.64 s, sys: 36.7 ms, total: 2.68 s\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "session_factory_arrow = DemoSQLSessionFactory(name=\"with arraw\")\n",
    "session_factory_arrow.add_config(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")\n",
    "%time test_performance(session_factory_arrow)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "常规的HelloWorld的example。\n",
    "页面上面的第一个例子。本质就是生成一个新的dataframe\n",
    "1. 在annotation上面列出的是新的dataframe的col和类型\n",
    "2. 他会自动的把pd的转换成spark的\n",
    "3. 函数应该会分批node执行。然后再汇总。因为我看到了。hello world的函数会被执行好几次"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+--------------------+\n",
      "|name|age|              salary|\n",
      "+----+---+--------------------+\n",
      "|   W|205| 0.04327623602354236|\n",
      "|   Y|958|  0.9260153652760735|\n",
      "|   H| 45| 0.06502276504831062|\n",
      "|   C|988|  0.3057879003138224|\n",
      "|   Y|530| 0.12418639405654952|\n",
      "|   H|689|  0.9823893447660684|\n",
      "|   E|902|  0.9772530265771143|\n",
      "|   J|659|  0.5908572731268977|\n",
      "|   W| 36|  0.7185792840437409|\n",
      "|   S|279|  0.8563242774434922|\n",
      "|   W|671|   0.827294413531321|\n",
      "|   V|698| 0.44847719327825175|\n",
      "|   X|829|  0.9563856623025289|\n",
      "|   O|422|  0.7659027237465561|\n",
      "|   R|556| 0.30253081988566743|\n",
      "|   W|710|0.005506032879962874|\n",
      "|   D|217| 0.13622538180912724|\n",
      "|   D|685| 0.32675390373979074|\n",
      "|   R|107| 0.15811854785577317|\n",
      "|   B|556|  0.9787700561733383|\n",
      "+----+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session_factory_arrow.add_config('spark.sql.execution.arrow.maxRecordsPerBatch',10)\n",
    "spark = session_factory_arrow.build_session()\n",
    "test_data = create_random_data(row_num=1000)\n",
    "basic_df = spark.createDataFrame(test_data,COLUMNS)\n",
    "basic_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              result|\n",
      "+--------------------+\n",
      "|[205.04327623602353]|\n",
      "|  [958.926015365276]|\n",
      "| [45.06502276504831]|\n",
      "| [988.3057879003138]|\n",
      "| [530.1241863940566]|\n",
      "|  [689.982389344766]|\n",
      "| [902.9772530265772]|\n",
      "| [659.5908572731269]|\n",
      "| [36.71857928404374]|\n",
      "| [279.8563242774435]|\n",
      "| [671.8272944135314]|\n",
      "| [698.4484771932782]|\n",
      "| [829.9563856623025]|\n",
      "|[422.76590272374654]|\n",
      "| [556.3025308198856]|\n",
      "| [710.0055060328799]|\n",
      "|[217.13622538180914]|\n",
      "| [685.3267539037398]|\n",
      "|[107.15811854785578]|\n",
      "| [556.9787700561733]|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"total double\")\n",
    "def func(s1: pd.Series, s2: pd.Series) -> pd.DataFrame:\n",
    "    print(\"execute\")\n",
    "    s3 = pd.DataFrame()\n",
    "    s3['total'] = s1 + s2\n",
    "    return s3\n",
    "basic_df.select(func(\"age\",\"salary\").alias(\"result\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "主要是想要看看。select方法，不能不能接受一个List"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|age|              salary|\n",
      "+---+--------------------+\n",
      "|205| 0.04327623602354236|\n",
      "|958|  0.9260153652760735|\n",
      "| 45| 0.06502276504831062|\n",
      "|988|  0.3057879003138224|\n",
      "|530| 0.12418639405654952|\n",
      "|689|  0.9823893447660684|\n",
      "|902|  0.9772530265771143|\n",
      "|659|  0.5908572731268977|\n",
      "| 36|  0.7185792840437409|\n",
      "|279|  0.8563242774434922|\n",
      "|671|   0.827294413531321|\n",
      "|698| 0.44847719327825175|\n",
      "|829|  0.9563856623025289|\n",
      "|422|  0.7659027237465561|\n",
      "|556| 0.30253081988566743|\n",
      "|710|0.005506032879962874|\n",
      "|217| 0.13622538180912724|\n",
      "|685| 0.32675390373979074|\n",
      "|107| 0.15811854785577317|\n",
      "|556|  0.9787700561733383|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def to_str_func(s1: pd.Series) -> pd.Series:\n",
    "    return s1.astype(dtype=str)\n",
    "to_str = pandas_udf(to_str_func, returnType=StringType())\n",
    "\n",
    "age_c = to_str(\"age\").alias(\"age\")\n",
    "salary_c = to_str(\"salary\").alias(\"salary\")\n",
    "selects = [age_c,salary_c]\n",
    "basic_df.select(selects).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "测试以下。如果参数是不定的行不行\n",
    "\n",
    "简单的来书，\n",
    "- 确定的column个数，用Series\n",
    "- 不确定用dataframe\n",
    "- iterator是类似用流"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|            result|\n",
      "+------------------+\n",
      "| 8.871628384826183|\n",
      "| 887.1227199344785|\n",
      "| 2.926024427173978|\n",
      "| 302.1184455100565|\n",
      "| 65.81878884997124|\n",
      "| 676.8662585438211|\n",
      "| 881.4822299725571|\n",
      "| 389.3749429906256|\n",
      "|25.868854225574673|\n",
      "|238.91447340673432|\n",
      "| 555.1145514795164|\n",
      "| 313.0370809082197|\n",
      "| 792.8437140487964|\n",
      "| 323.2109494210467|\n",
      "|168.20713585643108|\n",
      "| 3.909283344773641|\n",
      "|29.560907852580613|\n",
      "|223.82642406175665|\n",
      "| 16.91868462056773|\n",
      "| 544.1961512323761|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"double\")\n",
    "def to_sum_func(data: pd.DataFrame) -> pd.Series:\n",
    "    return data.age*data.salary\n",
    "cols = [col(\"age\"),col(\"salary\")]\n",
    "headers = struct(cols)\n",
    "#my_sum = pandas_udf(to_sum_func, returnType=DoubleType())\n",
    "basic_df.select(to_sum_func(headers).alias(\"result\")).show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "能不能用于SQL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|pandas_to_str(age)|\n",
      "+------------------+\n",
      "|               205|\n",
      "|               958|\n",
      "|                45|\n",
      "|               988|\n",
      "|               530|\n",
      "|               689|\n",
      "|               902|\n",
      "|               659|\n",
      "|                36|\n",
      "|               279|\n",
      "|               671|\n",
      "|               698|\n",
      "|               829|\n",
      "|               422|\n",
      "|               556|\n",
      "|               710|\n",
      "|               217|\n",
      "|               685|\n",
      "|               107|\n",
      "|               556|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "basic_df.createOrReplaceTempView(\"pandas_udf\")\n",
    "spark.udf.register(\"pandas_to_str\", to_str)\n",
    "spark.sql(\"select pandas_to_str(age) from pandas_udf\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "basic_df.createOrReplaceTempView(\"pandas_udf\")\n",
    "spark.udf.register(\"pandas_to_str\", to_str)\n",
    "spark.sql(\"select pandas_to_str(age) from pandas_udf\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`__call__`这个方法能不能用哪用"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|            result|\n",
      "+------------------+\n",
      "| 8.871628384826183|\n",
      "| 887.1227199344785|\n",
      "| 2.926024427173978|\n",
      "| 302.1184455100565|\n",
      "| 65.81878884997124|\n",
      "| 676.8662585438211|\n",
      "| 881.4822299725571|\n",
      "| 389.3749429906256|\n",
      "|25.868854225574673|\n",
      "|238.91447340673432|\n",
      "| 555.1145514795164|\n",
      "| 313.0370809082197|\n",
      "| 792.8437140487964|\n",
      "| 323.2109494210467|\n",
      "|168.20713585643108|\n",
      "| 3.909283344773641|\n",
      "|29.560907852580613|\n",
      "|223.82642406175665|\n",
      "| 16.91868462056773|\n",
      "| 544.1961512323761|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class PandasFunc:\n",
    "    def __call__(self, data: pd.DataFrame)-> pd.Series:\n",
    "         return data.age*data.salary\n",
    "\n",
    "cols = [col(\"age\"),col(\"salary\")]\n",
    "headers = struct(cols)\n",
    "class_my_sum = pandas_udf(PandasFunc(), returnType=DoubleType())\n",
    "basic_df.select(class_my_sum(headers).alias(\"result\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "返回多列的处理方法。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------------------+\n",
      "|age|              salary| col1|                col2|\n",
      "+---+--------------------+-----+--------------------+\n",
      "|205| 0.04327623602354236|205.0| 0.04327623602354236|\n",
      "|958|  0.9260153652760735|958.0|  0.9260153652760735|\n",
      "| 45| 0.06502276504831062| 45.0| 0.06502276504831062|\n",
      "|988|  0.3057879003138224|988.0|  0.3057879003138224|\n",
      "|530| 0.12418639405654952|530.0| 0.12418639405654952|\n",
      "|689|  0.9823893447660684|689.0|  0.9823893447660684|\n",
      "|902|  0.9772530265771143|902.0|  0.9772530265771143|\n",
      "|659|  0.5908572731268977|659.0|  0.5908572731268977|\n",
      "| 36|  0.7185792840437409| 36.0|  0.7185792840437409|\n",
      "|279|  0.8563242774434922|279.0|  0.8563242774434922|\n",
      "|671|   0.827294413531321|671.0|   0.827294413531321|\n",
      "|698| 0.44847719327825175|698.0| 0.44847719327825175|\n",
      "|829|  0.9563856623025289|829.0|  0.9563856623025289|\n",
      "|422|  0.7659027237465561|422.0|  0.7659027237465561|\n",
      "|556| 0.30253081988566743|556.0| 0.30253081988566743|\n",
      "|710|0.005506032879962874|710.0|0.005506032879962874|\n",
      "|217| 0.13622538180912724|217.0| 0.13622538180912724|\n",
      "|685| 0.32675390373979074|685.0| 0.32675390373979074|\n",
      "|107| 0.15811854785577317|107.0| 0.15811854785577317|\n",
      "|556|  0.9787700561733383|556.0|  0.9787700561733383|\n",
      "+---+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"col1 double, col2 double\")\n",
    "def to_multi_return_func(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"execute\")\n",
    "    s3 = pd.DataFrame()\n",
    "    s3['col1'] = data.age\n",
    "    s3['col2'] = data.salary\n",
    "    return s3\n",
    "cols = [col(\"age\"),col(\"salary\")]\n",
    "headers = struct(cols)\n",
    "#my_sum = pandas_udf(to_sum_func, returnType=DoubleType())\n",
    "multi_return_df = basic_df.withColumn(\"abc\",to_multi_return_func(headers))\n",
    "multi_return_df.select(col(\"age\"),col(\"salary\"),col(\"abc.col1\"),col(\"abc.col2\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里的目的，主要还是为了验证一下partitionBy的用法\n",
    "\n",
    "[normalize pyspark dataframe by group](https://stackoverflow.com/questions/54112439/normalize-pyspark-data-frame-by-group)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|value|\n",
      "+----+-----+\n",
      "|   A|    0|\n",
      "|   A|    1|\n",
      "|   A|    2|\n",
      "|   A|    3|\n",
      "|   A|    4|\n",
      "|   A|    5|\n",
      "|   A|    6|\n",
      "|   A|    7|\n",
      "|   A|    8|\n",
      "|   A|    9|\n",
      "|   B|   10|\n",
      "|   B|   11|\n",
      "|   B|   12|\n",
      "|   B|   13|\n",
      "|   B|   14|\n",
      "|   B|   15|\n",
      "|   B|   16|\n",
      "|   B|   17|\n",
      "|   B|   18|\n",
      "|   B|   19|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partition_key_data = [('A',x) for x in range(10)]+[('B',x) for x in range(10,20)]\n",
    "partition_df = spark.createDataFrame(partition_key_data,[\"name\",\"value\"]).cache()\n",
    "partition_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandlersong/oneDrive/code/pyspark/venv/lib/python3.7/site-packages/pyspark/sql/pandas/group_ops.py:76: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"more details.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+\n",
      "|                 new|name|value|\n",
      "+--------------------+----+-----+\n",
      "| -1.4863010829205867|   B| 10.0|\n",
      "| -1.1560119533826787|   B| 11.0|\n",
      "| -0.8257228238447705|   B| 12.0|\n",
      "|-0.49543369430686224|   B| 13.0|\n",
      "| -0.1651445647689541|   B| 14.0|\n",
      "|  0.1651445647689541|   B| 15.0|\n",
      "| 0.49543369430686224|   B| 16.0|\n",
      "|  0.8257228238447705|   B| 17.0|\n",
      "|  1.1560119533826787|   B| 18.0|\n",
      "|  1.4863010829205867|   B| 19.0|\n",
      "| -1.4863010829205867|   A|  0.0|\n",
      "| -1.1560119533826787|   A|  1.0|\n",
      "| -0.8257228238447705|   A|  2.0|\n",
      "|-0.49543369430686224|   A|  3.0|\n",
      "| -0.1651445647689541|   A|  4.0|\n",
      "|  0.1651445647689541|   A|  5.0|\n",
      "| 0.49543369430686224|   A|  6.0|\n",
      "|  0.8257228238447705|   A|  7.0|\n",
      "|  1.1560119533826787|   A|  8.0|\n",
      "|  1.4863010829205867|   A|  9.0|\n",
      "+--------------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"new double,name string,value double\",functionType=PandasUDFType.GROUPED_MAP)\n",
    "def group_by_normalize(data) -> pd.DataFrame:\n",
    "    value = data[\"value\"]\n",
    "    df = (value - value.mean())/value.std()\n",
    "    data['new'] = df\n",
    "    return data\n",
    "partition_df.groupby(\"name\").apply(group_by_normalize).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "follow can't work\n",
    "```python\n",
    "import numpy as np\n",
    "w = Window.partitionBy('name')\n",
    "@pandas_udf(\"new double\",functionType=PandasUDFType.GROUPED_MAP)\n",
    "def group_by_normalize_2(data:pd.Series) -> pd.DataFrame:\n",
    "    norm = (data - data.mean())/data.std()\n",
    "    res = pd.DataFrame\n",
    "    res[\"new\"] = norm\n",
    "    print(res)\n",
    "    return res\n",
    "\n",
    "partition_df.withColumn(\"new\",group_by_normalize_2(col(\"value\")).over(w)).show()\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}