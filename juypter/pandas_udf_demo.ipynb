{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from pyspark.sql.pandas.functions import pandas_udf\n",
    "from sparkstudy.deploy.demo_sessions import DemoSQLSessionFactory\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "COLUMNS = [\"name\",\"age\",\"salary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "比较下开不开启arrow的区别\n",
    "\n",
    "测试下来，感觉性能提升有点奇怪。有时候会快，有时候会慢。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def create_random_data(row_num:int)->List[tuple]:\n",
    "     result = list()\n",
    "     a_str = string.ascii_uppercase\n",
    "     for i in range(row_num):\n",
    "         random_letter = random.choice(a_str)\n",
    "         result.append((random_letter,random.randint(1,row_num),random.random()))\n",
    "     return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def test_performance(session_factory:DemoSQLSessionFactory, n:int = 100000):\n",
    "    data = create_random_data(n)\n",
    "    spark_session = session_factory.build_session()\n",
    "    df = spark_session.createDataFrame(data,COLUMNS).cache()\n",
    "    df.toPandas().head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.09 s, sys: 50.5 ms, total: 3.14 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "session_factory_normal = DemoSQLSessionFactory(name=\"normal\")\n",
    "%time test_performance(session_factory_normal)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.77 s, sys: 19.5 ms, total: 2.78 s\n",
      "Wall time: 3.29 s\n"
     ]
    }
   ],
   "source": [
    "session_factory_arrow = DemoSQLSessionFactory(name=\"with arraw\")\n",
    "session_factory_arrow.add_config(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")\n",
    "%time test_performance(session_factory_arrow)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "常规的HelloWorld的example。\n",
    "页面上面的第一个例子。本质就是生成一个新的dataframe\n",
    "1. 在annotation上面列出的是新的dataframe的col和类型\n",
    "2. 他会自动的把pd的转换成spark的\n",
    "3. 函数应该会分批node执行。然后再汇总。因为我看到了。hello world的函数会被执行好几次"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------------------+\n",
      "|name| age|              salary|\n",
      "+----+----+--------------------+\n",
      "|   H|1000|  0.6615702849667711|\n",
      "|   Y| 137| 0.49502192370290155|\n",
      "|   F| 306| 0.24357611005187163|\n",
      "|   U| 265|   0.611489662174653|\n",
      "|   O|   5| 0.19890048211207068|\n",
      "|   I|  85|  0.5142452856852364|\n",
      "|   L| 497|  0.8521695721702193|\n",
      "|   T| 233|  0.5205520446495158|\n",
      "|   M| 601|0.021362590303023565|\n",
      "|   C| 509| 0.06582586205632968|\n",
      "|   L| 281| 0.40850737166518314|\n",
      "|   X| 958|  0.5662043110891974|\n",
      "|   D| 124|0.014962613865048158|\n",
      "|   H| 769|0.016595441983602077|\n",
      "|   S| 330| 0.24779907845329496|\n",
      "|   P| 185|  0.7396863119855275|\n",
      "|   L| 214|  0.6983798071301994|\n",
      "|   T| 495|  0.8820863717262004|\n",
      "|   R|  40|  0.9404487883934665|\n",
      "|   I| 234|  0.8859580362471405|\n",
      "+----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session_factory_arrow.add_config('spark.sql.execution.arrow.maxRecordsPerBatch',10)\n",
    "spark = session_factory_arrow.build_session()\n",
    "test_data = create_random_data(row_num=1000)\n",
    "basic_df = spark.createDataFrame(test_data,COLUMNS)\n",
    "basic_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              result|\n",
      "+--------------------+\n",
      "|[1000.6615702849667]|\n",
      "| [137.4950219237029]|\n",
      "| [306.2435761100519]|\n",
      "| [265.6114896621747]|\n",
      "|  [5.19890048211207]|\n",
      "| [85.51424528568523]|\n",
      "| [497.8521695721702]|\n",
      "|[233.52055204464952]|\n",
      "|  [601.021362590303]|\n",
      "|[509.06582586205633]|\n",
      "|[281.40850737166517]|\n",
      "| [958.5662043110892]|\n",
      "|[124.01496261386505]|\n",
      "| [769.0165954419836]|\n",
      "| [330.2477990784533]|\n",
      "|[185.73968631198554]|\n",
      "| [214.6983798071302]|\n",
      "| [495.8820863717262]|\n",
      "| [40.94044878839347]|\n",
      "|[234.88595803624713]|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"total double\")\n",
    "def func(s1: pd.Series, s2: pd.Series) -> pd.DataFrame:\n",
    "    print(\"execute\")\n",
    "    s3 = pd.DataFrame()\n",
    "    s3['total'] = s1 + s2\n",
    "    return s3\n",
    "basic_df.select(func(\"age\",\"salary\").alias(\"result\")).show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}