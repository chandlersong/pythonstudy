{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "from typing import List,Iterator,Tuple\n",
    "from pyspark.sql.pandas.functions import pandas_udf\n",
    "from pyspark.sql.functions import struct, col\n",
    "\n",
    "from pyspark.sql.types import StringType,DoubleType\n",
    "from sparkstudy.deploy.demo_sessions import DemoSQLSessionFactory\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "COLUMNS = [\"name\",\"age\",\"salary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "比较下开不开启arrow的区别\n",
    "\n",
    "测试下来，感觉性能提升有点奇怪。有时候会快，有时候会慢。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def create_random_data(row_num:int)->List[tuple]:\n",
    "     result = list()\n",
    "     a_str = string.ascii_uppercase\n",
    "     for i in range(row_num):\n",
    "         random_letter = random.choice(a_str)\n",
    "         result.append((random_letter,random.randint(1,row_num),random.random()))\n",
    "     return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def test_performance(session_factory:DemoSQLSessionFactory, n:int = 100000):\n",
    "    data = create_random_data(n)\n",
    "    spark_session = session_factory.build_session()\n",
    "    df = spark_session.createDataFrame(data,COLUMNS).cache()\n",
    "    df.toPandas().head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.05 s, sys: 51.7 ms, total: 3.1 s\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "session_factory_normal = DemoSQLSessionFactory(name=\"normal\")\n",
    "%time test_performance(session_factory_normal)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.84 s, sys: 22.3 ms, total: 2.86 s\n",
      "Wall time: 3.41 s\n"
     ]
    }
   ],
   "source": [
    "session_factory_arrow = DemoSQLSessionFactory(name=\"with arraw\")\n",
    "session_factory_arrow.add_config(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")\n",
    "%time test_performance(session_factory_arrow)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "常规的HelloWorld的example。\n",
    "页面上面的第一个例子。本质就是生成一个新的dataframe\n",
    "1. 在annotation上面列出的是新的dataframe的col和类型\n",
    "2. 他会自动的把pd的转换成spark的\n",
    "3. 函数应该会分批node执行。然后再汇总。因为我看到了。hello world的函数会被执行好几次"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-------------------+\n",
      "|name|age|             salary|\n",
      "+----+---+-------------------+\n",
      "|   I|625| 0.7905260172728258|\n",
      "|   L|121|0.15568573810904118|\n",
      "|   C|507|  0.773368400830384|\n",
      "|   I|741| 0.5315484410622431|\n",
      "|   P|967| 0.7963605997989015|\n",
      "|   E| 97| 0.8525953416181977|\n",
      "|   C|447|0.24632034189710417|\n",
      "|   J|132| 0.7617200252444477|\n",
      "|   E|276|0.35385200538445893|\n",
      "|   E|112| 0.6746114657290763|\n",
      "|   J|466| 0.8844738355236132|\n",
      "|   S|644|0.07209591236191981|\n",
      "|   C|662|   0.92176358697027|\n",
      "|   L|612|0.23139110708692412|\n",
      "|   U|638| 0.2426797460194945|\n",
      "|   I| 96| 0.5869054936620904|\n",
      "|   T|745|0.47041040019443814|\n",
      "|   J|687| 0.3531786496215842|\n",
      "|   L|155| 0.1903792605123351|\n",
      "|   K|649|0.18742033516472556|\n",
      "+----+---+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session_factory_arrow.add_config('spark.sql.execution.arrow.maxRecordsPerBatch',10)\n",
    "spark = session_factory_arrow.build_session()\n",
    "test_data = create_random_data(row_num=1000)\n",
    "basic_df = spark.createDataFrame(test_data,COLUMNS)\n",
    "basic_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              result|\n",
      "+--------------------+\n",
      "| [625.7905260172728]|\n",
      "|[121.15568573810904]|\n",
      "| [507.7733684008304]|\n",
      "| [741.5315484410622]|\n",
      "| [967.7963605997988]|\n",
      "|  [97.8525953416182]|\n",
      "| [447.2463203418971]|\n",
      "|[132.76172002524444]|\n",
      "|[276.35385200538445]|\n",
      "|[112.67461146572907]|\n",
      "|[466.88447383552364]|\n",
      "| [644.0720959123619]|\n",
      "| [662.9217635869703]|\n",
      "| [612.2313911070869]|\n",
      "| [638.2426797460195]|\n",
      "| [96.58690549366209]|\n",
      "| [745.4704104001944]|\n",
      "| [687.3531786496216]|\n",
      "|[155.19037926051234]|\n",
      "| [649.1874203351647]|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"total double\")\n",
    "def func(s1: pd.Series, s2: pd.Series) -> pd.DataFrame:\n",
    "    print(\"execute\")\n",
    "    s3 = pd.DataFrame()\n",
    "    s3['total'] = s1 + s2\n",
    "    return s3\n",
    "basic_df.select(func(\"age\",\"salary\").alias(\"result\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "主要是想要看看。select方法，不能不能接受一个List"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "|age|             salary|\n",
      "+---+-------------------+\n",
      "|625| 0.7905260172728258|\n",
      "|121|0.15568573810904118|\n",
      "|507|  0.773368400830384|\n",
      "|741| 0.5315484410622431|\n",
      "|967| 0.7963605997989015|\n",
      "| 97| 0.8525953416181977|\n",
      "|447|0.24632034189710417|\n",
      "|132| 0.7617200252444477|\n",
      "|276|0.35385200538445893|\n",
      "|112| 0.6746114657290763|\n",
      "|466| 0.8844738355236132|\n",
      "|644|0.07209591236191981|\n",
      "|662|   0.92176358697027|\n",
      "|612|0.23139110708692412|\n",
      "|638| 0.2426797460194945|\n",
      "| 96| 0.5869054936620904|\n",
      "|745|0.47041040019443814|\n",
      "|687| 0.3531786496215842|\n",
      "|155| 0.1903792605123351|\n",
      "|649|0.18742033516472556|\n",
      "+---+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def to_str_func(s1: pd.Series) -> pd.Series:\n",
    "    return s1.astype(dtype=str)\n",
    "to_str = pandas_udf(to_str_func, returnType=StringType())\n",
    "\n",
    "age_c = to_str(\"age\").alias(\"age\")\n",
    "salary_c = to_str(\"salary\").alias(\"salary\")\n",
    "selects = [age_c,salary_c]\n",
    "basic_df.select(selects).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "测试以下。如果参数是不定的行不行\n",
    "\n",
    "简单的来书，\n",
    "- 确定的column个数，用Series\n",
    "- 不确定用dataframe\n",
    "- iterator是类似用流"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|            result|\n",
      "+------------------+\n",
      "|494.07876079551613|\n",
      "| 18.83797431119398|\n",
      "| 392.0977792210047|\n",
      "|393.87739482712215|\n",
      "| 770.0807000055378|\n",
      "| 82.70174813696518|\n",
      "|110.10519282800556|\n",
      "| 100.5470433322671|\n",
      "| 97.66315348611067|\n",
      "| 75.55648416165654|\n",
      "|412.16480735400376|\n",
      "| 46.42976756107636|\n",
      "| 610.2074945743187|\n",
      "|141.61135753719756|\n",
      "| 154.8296779604375|\n",
      "| 56.34292739156068|\n",
      "| 350.4557481448564|\n",
      "|242.63373229002835|\n",
      "|29.508785379411943|\n",
      "| 121.6357975219069|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"double\")\n",
    "def to_sum_func(data: pd.DataFrame) -> pd.Series:\n",
    "    return data.age*data.salary\n",
    "cols = [col(\"age\"),col(\"salary\")]\n",
    "headers = struct(cols)\n",
    "#my_sum = pandas_udf(to_sum_func, returnType=DoubleType())\n",
    "basic_df.select(to_sum_func(headers).alias(\"result\")).show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}