{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import struct, col\n",
    "from pyspark.sql.pandas.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "\n",
    "from sparkstudy.deploy.demo_sessions import DemoSQLSessionFactory\n",
    "from sparkstudy.libs.tools import create_random_data\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "COLUMNS = [\"name\",\"age\",\"salary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "比较下开不开启arrow的区别\n",
    "\n",
    "测试下来，感觉性能提升有点奇怪。有时候会快，有时候会慢。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def test_performance(session_factory:DemoSQLSessionFactory, n:int = 100000):\n",
    "    data = create_random_data(n)\n",
    "    spark_session = session_factory.build_session()\n",
    "    df = spark_session.createDataFrame(data,COLUMNS).cache()\n",
    "    df.toPandas().head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.59 s, sys: 39 ms, total: 2.63 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "session_factory_arrow = DemoSQLSessionFactory(name=\"with arraw\")\n",
    "session_factory_arrow.add_config(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")\n",
    "%time test_performance(session_factory_arrow)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "常规的HelloWorld的example。\n",
    "页面上面的第一个例子。本质就是生成一个新的dataframe\n",
    "1. 在annotation上面列出的是新的dataframe的col和类型\n",
    "2. 他会自动的把pd的转换成spark的\n",
    "3. 函数应该会分批node执行。然后再汇总。因为我看到了。hello world的函数会被执行好几次"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-------------------+\n",
      "|name|age|             salary|\n",
      "+----+---+-------------------+\n",
      "|   P|559|0.16409891492104978|\n",
      "|   J|244|0.14893765822564886|\n",
      "|   T|825| 0.8465170954153429|\n",
      "|   L|617|0.24441519974300752|\n",
      "|   B|369|0.05997399126491576|\n",
      "|   A|820| 0.6820136758134797|\n",
      "|   W|710|0.33806890904676257|\n",
      "|   Q|869| 0.5121709882107319|\n",
      "|   E|234|0.11226645551188152|\n",
      "|   I|169| 0.3030591606796864|\n",
      "|   E|436| 0.7236067744870015|\n",
      "|   T|522|0.37965515031735997|\n",
      "|   N|525|0.21233611808070207|\n",
      "|   C|353|0.04290133090471937|\n",
      "|   C| 93|0.06170264673252168|\n",
      "|   L|235|0.07863983381189388|\n",
      "|   X| 51| 0.6094419787359339|\n",
      "|   D|923|0.45009256026484823|\n",
      "|   L|185|0.20790219324212023|\n",
      "|   O| 78| 0.7171646379115556|\n",
      "+----+---+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session_factory_arrow.add_config('spark.sql.execution.arrow.maxRecordsPerBatch',10)\n",
    "spark = session_factory_arrow.build_session()\n",
    "test_data = create_random_data(row_num=1000)\n",
    "basic_df = spark.createDataFrame(test_data,COLUMNS)\n",
    "basic_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              result|\n",
      "+--------------------+\n",
      "|  [559.164098914921]|\n",
      "|[244.14893765822566]|\n",
      "| [825.8465170954154]|\n",
      "|  [617.244415199743]|\n",
      "|[369.05997399126494]|\n",
      "| [820.6820136758134]|\n",
      "| [710.3380689090468]|\n",
      "| [869.5121709882108]|\n",
      "|[234.11226645551187]|\n",
      "|[169.30305916067968]|\n",
      "|  [436.723606774487]|\n",
      "| [522.3796551503174]|\n",
      "| [525.2123361180807]|\n",
      "| [353.0429013309047]|\n",
      "| [93.06170264673253]|\n",
      "| [235.0786398338119]|\n",
      "| [51.60944197873594]|\n",
      "| [923.4500925602648]|\n",
      "|[185.20790219324212]|\n",
      "| [78.71716463791155]|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"total double\")\n",
    "def func(s1: pd.Series, s2: pd.Series) -> pd.DataFrame:\n",
    "    print(\"execute\")\n",
    "    s3 = pd.DataFrame()\n",
    "    s3['total'] = s1 + s2\n",
    "    return s3\n",
    "basic_df.select(func(\"age\",\"salary\").alias(\"result\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "主要是想要看看。select方法，不能不能接受一个List"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "|age|             salary|\n",
      "+---+-------------------+\n",
      "|559|0.16409891492104978|\n",
      "|244|0.14893765822564886|\n",
      "|825| 0.8465170954153429|\n",
      "|617|0.24441519974300752|\n",
      "|369|0.05997399126491576|\n",
      "|820| 0.6820136758134797|\n",
      "|710|0.33806890904676257|\n",
      "|869| 0.5121709882107319|\n",
      "|234|0.11226645551188152|\n",
      "|169| 0.3030591606796864|\n",
      "|436| 0.7236067744870015|\n",
      "|522|0.37965515031735997|\n",
      "|525|0.21233611808070207|\n",
      "|353|0.04290133090471937|\n",
      "| 93|0.06170264673252168|\n",
      "|235|0.07863983381189388|\n",
      "| 51| 0.6094419787359339|\n",
      "|923|0.45009256026484823|\n",
      "|185|0.20790219324212023|\n",
      "| 78| 0.7171646379115556|\n",
      "+---+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def to_str_func(s1: pd.Series) -> pd.Series:\n",
    "    return s1.astype(dtype=str)\n",
    "to_str = pandas_udf(to_str_func, returnType=StringType())\n",
    "\n",
    "age_c = to_str(\"age\").alias(\"age\")\n",
    "salary_c = to_str(\"salary\").alias(\"salary\")\n",
    "selects = [age_c,salary_c]\n",
    "basic_df.select(selects).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "测试以下。如果参数是不定的行不行\n",
    "\n",
    "简单的来书，\n",
    "- 确定的column个数，用Series\n",
    "- 不确定用dataframe\n",
    "- iterator是类似用流"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|            result|\n",
      "+------------------+\n",
      "| 91.73129344086682|\n",
      "| 36.34078860705832|\n",
      "| 698.3766037176579|\n",
      "|150.80417824143564|\n",
      "|22.130402776753915|\n",
      "| 559.2512141670534|\n",
      "|240.02892542320143|\n",
      "|445.07658875512607|\n",
      "| 26.27035058978028|\n",
      "| 51.21699815486701|\n",
      "|315.49255367633265|\n",
      "| 198.1799884656619|\n",
      "|111.47646199236858|\n",
      "|15.144169809365938|\n",
      "| 5.738346146124517|\n",
      "|18.480360945795063|\n",
      "|31.081540915532628|\n",
      "| 415.4354331244549|\n",
      "|38.461905749792244|\n",
      "|55.938841757101336|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"double\")\n",
    "def to_sum_func(data: pd.DataFrame) -> pd.Series:\n",
    "    return data.age*data.salary\n",
    "cols = [col(\"age\"),col(\"salary\")]\n",
    "headers = struct(cols)\n",
    "#my_sum = pandas_udf(to_sum_func, returnType=DoubleType())\n",
    "basic_df.select(to_sum_func(headers).alias(\"result\")).show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "能不能用于SQL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|pandas_to_str(age)|\n",
      "+------------------+\n",
      "|               559|\n",
      "|               244|\n",
      "|               825|\n",
      "|               617|\n",
      "|               369|\n",
      "|               820|\n",
      "|               710|\n",
      "|               869|\n",
      "|               234|\n",
      "|               169|\n",
      "|               436|\n",
      "|               522|\n",
      "|               525|\n",
      "|               353|\n",
      "|                93|\n",
      "|               235|\n",
      "|                51|\n",
      "|               923|\n",
      "|               185|\n",
      "|                78|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "basic_df.createOrReplaceTempView(\"pandas_udf\")\n",
    "spark.udf.register(\"pandas_to_str\", to_str)\n",
    "spark.sql(\"select pandas_to_str(age) from pandas_udf\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "basic_df.createOrReplaceTempView(\"pandas_udf\")\n",
    "spark.udf.register(\"pandas_to_str\", to_str)\n",
    "spark.sql(\"select pandas_to_str(age) from pandas_udf\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`__call__`这个方法能不能用哪用"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|            result|\n",
      "+------------------+\n",
      "| 91.73129344086682|\n",
      "| 36.34078860705832|\n",
      "| 698.3766037176579|\n",
      "|150.80417824143564|\n",
      "|22.130402776753915|\n",
      "| 559.2512141670534|\n",
      "|240.02892542320143|\n",
      "|445.07658875512607|\n",
      "| 26.27035058978028|\n",
      "| 51.21699815486701|\n",
      "|315.49255367633265|\n",
      "| 198.1799884656619|\n",
      "|111.47646199236858|\n",
      "|15.144169809365938|\n",
      "| 5.738346146124517|\n",
      "|18.480360945795063|\n",
      "|31.081540915532628|\n",
      "| 415.4354331244549|\n",
      "|38.461905749792244|\n",
      "|55.938841757101336|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class PandasFunc:\n",
    "    def __call__(self, data: pd.DataFrame)-> pd.Series:\n",
    "         return data.age*data.salary\n",
    "\n",
    "cols = [col(\"age\"),col(\"salary\")]\n",
    "headers = struct(cols)\n",
    "class_my_sum = pandas_udf(PandasFunc(), returnType=DoubleType())\n",
    "basic_df.select(class_my_sum(headers).alias(\"result\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "返回多列的处理方法。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----+-------------------+\n",
      "|age|             salary| col1|               col2|\n",
      "+---+-------------------+-----+-------------------+\n",
      "|559|0.16409891492104978|559.0|0.16409891492104978|\n",
      "|244|0.14893765822564886|244.0|0.14893765822564886|\n",
      "|825| 0.8465170954153429|825.0| 0.8465170954153429|\n",
      "|617|0.24441519974300752|617.0|0.24441519974300752|\n",
      "|369|0.05997399126491576|369.0|0.05997399126491576|\n",
      "|820| 0.6820136758134797|820.0| 0.6820136758134797|\n",
      "|710|0.33806890904676257|710.0|0.33806890904676257|\n",
      "|869| 0.5121709882107319|869.0| 0.5121709882107319|\n",
      "|234|0.11226645551188152|234.0|0.11226645551188152|\n",
      "|169| 0.3030591606796864|169.0| 0.3030591606796864|\n",
      "|436| 0.7236067744870015|436.0| 0.7236067744870015|\n",
      "|522|0.37965515031735997|522.0|0.37965515031735997|\n",
      "|525|0.21233611808070207|525.0|0.21233611808070207|\n",
      "|353|0.04290133090471937|353.0|0.04290133090471937|\n",
      "| 93|0.06170264673252168| 93.0|0.06170264673252168|\n",
      "|235|0.07863983381189388|235.0|0.07863983381189388|\n",
      "| 51| 0.6094419787359339| 51.0| 0.6094419787359339|\n",
      "|923|0.45009256026484823|923.0|0.45009256026484823|\n",
      "|185|0.20790219324212023|185.0|0.20790219324212023|\n",
      "| 78| 0.7171646379115556| 78.0| 0.7171646379115556|\n",
      "+---+-------------------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"col1 double, col2 double\")\n",
    "def to_multi_return_func(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"execute\")\n",
    "    s3 = pd.DataFrame()\n",
    "    s3['col1'] = data.age\n",
    "    s3['col2'] = data.salary\n",
    "    return s3\n",
    "cols = [col(\"age\"),col(\"salary\")]\n",
    "headers = struct(cols)\n",
    "#my_sum = pandas_udf(to_sum_func, returnType=DoubleType())\n",
    "multi_return_df = basic_df.withColumn(\"abc\",to_multi_return_func(headers))\n",
    "multi_return_df.select(col(\"age\"),col(\"salary\"),col(\"abc.col1\"),col(\"abc.col2\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里的目的，主要还是为了验证一下partitionBy的用法\n",
    "\n",
    "[normalize pyspark dataframe by group](https://stackoverflow.com/questions/54112439/normalize-pyspark-data-frame-by-group)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|value|\n",
      "+----+-----+\n",
      "|   A|    0|\n",
      "|   A|    1|\n",
      "|   A|    2|\n",
      "|   A|    3|\n",
      "|   A|    4|\n",
      "|   A|    5|\n",
      "|   A|    6|\n",
      "|   A|    7|\n",
      "|   A|    8|\n",
      "|   A|    9|\n",
      "|   B|   10|\n",
      "|   B|   11|\n",
      "|   B|   12|\n",
      "|   B|   13|\n",
      "|   B|   14|\n",
      "|   B|   15|\n",
      "|   B|   16|\n",
      "|   B|   17|\n",
      "|   B|   18|\n",
      "|   B|   19|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partition_key_data = [('A',x) for x in range(10)]+[('B',x) for x in range(10,20)]\n",
    "partition_df = spark.createDataFrame(partition_key_data,[\"name\",\"value\"]).cache()\n",
    "partition_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandlersong/oneDrive/code/pyspark/venv/lib/python3.7/site-packages/pyspark/sql/pandas/group_ops.py:76: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"more details.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+\n",
      "|                 new|name|value|\n",
      "+--------------------+----+-----+\n",
      "| -1.4863010829205867|   B| 10.0|\n",
      "| -1.1560119533826787|   B| 11.0|\n",
      "| -0.8257228238447705|   B| 12.0|\n",
      "|-0.49543369430686224|   B| 13.0|\n",
      "| -0.1651445647689541|   B| 14.0|\n",
      "|  0.1651445647689541|   B| 15.0|\n",
      "| 0.49543369430686224|   B| 16.0|\n",
      "|  0.8257228238447705|   B| 17.0|\n",
      "|  1.1560119533826787|   B| 18.0|\n",
      "|  1.4863010829205867|   B| 19.0|\n",
      "| -1.4863010829205867|   A|  0.0|\n",
      "| -1.1560119533826787|   A|  1.0|\n",
      "| -0.8257228238447705|   A|  2.0|\n",
      "|-0.49543369430686224|   A|  3.0|\n",
      "| -0.1651445647689541|   A|  4.0|\n",
      "|  0.1651445647689541|   A|  5.0|\n",
      "| 0.49543369430686224|   A|  6.0|\n",
      "|  0.8257228238447705|   A|  7.0|\n",
      "|  1.1560119533826787|   A|  8.0|\n",
      "|  1.4863010829205867|   A|  9.0|\n",
      "+--------------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"new double,name string,value double\",functionType=PandasUDFType.GROUPED_MAP)\n",
    "def group_by_normalize(data) -> pd.DataFrame:\n",
    "    value = data[\"value\"]\n",
    "    df = (value - value.mean())/value.std()\n",
    "    data['new'] = df\n",
    "    return data\n",
    "\n",
    "partition_df.groupby(\"name\").apply(group_by_normalize).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}