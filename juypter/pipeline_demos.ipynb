{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from sparkstudy.deploy.demo_sessions import DemoSQLSessionFactory\n",
    "from sparkstudy.ml.transformers import DemoTransformer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "最简单的hello world例子。也没有什么太特别的东西。\n",
    "1. pipeline和scipy的有一点不一样。这里的fit和transformer是强行分开的。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "sessionFactory = DemoSQLSessionFactory(name=\"pipeline exampe\")\n",
    "spark = sessionFactory.build_session()\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+--------------------+\n",
      "| id|            text|label|               words|\n",
      "+---+----------------+-----+--------------------+\n",
      "|  0| a b c d e spark|  1.0|[a, b, c, d, e, s...|\n",
      "|  1|             b d|  0.0|              [b, d]|\n",
      "|  2|     spark f g h|  1.0|    [spark, f, g, h]|\n",
      "|  3|hadoop mapreduce|  0.0| [hadoop, mapreduce]|\n",
      "+---+----------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer])\n",
    "model = pipeline.fit(training)\n",
    "model.transform(training).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "主要看看有没有任何嵌套。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+--------------------+--------------------+\n",
      "| id|            text|label|             words_1|               words|\n",
      "+---+----------------+-----+--------------------+--------------------+\n",
      "|  0| a b c d e spark|  1.0|[a, b, c, d, e, s...|[a, b, c, d, e, s...|\n",
      "|  1|             b d|  0.0|              [b, d]|              [b, d]|\n",
      "|  2|     spark f g h|  1.0|    [spark, f, g, h]|    [spark, f, g, h]|\n",
      "|  3|hadoop mapreduce|  0.0| [hadoop, mapreduce]| [hadoop, mapreduce]|\n",
      "+---+----------------+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenize_1 = Tokenizer(inputCol=\"text\", outputCol=\"words_1\")\n",
    "pipeline_1 = Pipeline(stages=[tokenize_1,pipeline])\n",
    "pipeline_1.fit(training).transform(training).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "自定义的transformer\n",
    "1. 感觉相对来说，还是比较简单的。就是处理transformer的一个方法。而且传入的是全集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+---+\n",
      "| id|            text|label|new|\n",
      "+---+----------------+-----+---+\n",
      "|  0| a b c d e spark|  1.0| 15|\n",
      "|  1|             b d|  0.0|  3|\n",
      "|  2|     spark f g h|  1.0| 11|\n",
      "|  3|hadoop mapreduce|  0.0| 16|\n",
      "+---+----------------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_transformer = DemoTransformer()\n",
    "custom_pipeline = Pipeline(stages=[custom_transformer])\n",
    "custom_pipeline.fit(training).transform(training).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}